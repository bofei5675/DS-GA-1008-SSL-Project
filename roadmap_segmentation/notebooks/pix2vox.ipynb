{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:08:53.228089Z",
     "start_time": "2020-04-25T22:08:53.222628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code Reference https://github.com/hzxie/Pix2Vox/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T23:01:52.220359Z",
     "start_time": "2020-04-25T23:01:52.197730Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, cfg, pretrained=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Layer Definition\n",
    "        resnet50 = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        self.resnet = torch.nn.Sequential(*list(resnet50.children()))[:-3]\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1024, 512, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ELU(),\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=1),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 256, kernel_size=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Don't update params in ResNet\n",
    "        for param in resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, rendering_images):\n",
    "        # print(rendering_images.size())  # torch.Size([batch_size, n_views, img_c, img_h, img_w])\n",
    "        rendering_images = rendering_images.permute(1, 0, 2, 3, 4).contiguous()\n",
    "        rendering_images = torch.split(rendering_images, 1, dim=0)\n",
    "        image_features = []\n",
    "\n",
    "        for img in rendering_images:\n",
    "            features = self.resnet(img.squeeze(dim=0))\n",
    "            # print(features.size())    # torch.Size([batch_size, 512, 26, 26])\n",
    "            features = self.layer1(features)\n",
    "            # print(features.size())    # torch.Size([batch_size, 512, 24, 24])\n",
    "            features = self.layer2(features)\n",
    "            # print(features.size())    # torch.Size([batch_size, 512, 8, 8])\n",
    "            features = self.layer3(features)\n",
    "            # print(features.size())    # torch.Size([batch_size, 256, 8, 8])\n",
    "            image_features.append(features)\n",
    "\n",
    "        image_features = torch.stack(image_features).permute(1, 0, 2, 3, 4).contiguous()\n",
    "        # print(image_features.size())  # torch.Size([batch_size, n_views, 256, 8, 8])\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:08:25.124678Z",
     "start_time": "2020-04-25T22:08:25.118849Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:17:28.171588Z",
     "start_time": "2020-04-25T22:17:27.431116Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Encoder(cfg=None).cuda()\n",
    "# input size = [batch_size, n_views, img_c, img_h, img_w]\n",
    "encoder_outputs =model(torch.Tensor(1, 6, 3, 416, 416).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:10:40.041273Z",
     "start_time": "2020-04-25T22:10:40.014615Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Layer Definition\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(2048, 512, kernel_size=4, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm3d(512),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(512, 128, kernel_size=4, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm3d(128),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(128, 32, kernel_size=4, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm3d(32),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(32, 8, kernel_size=4, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm3d(8),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(8, 1, kernel_size=1, bias=True),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        image_features = image_features.permute(1, 0, 2, 3, 4).contiguous()\n",
    "        image_features = torch.split(image_features, 1, dim=0)\n",
    "        gen_volumes = []\n",
    "        raw_features = []\n",
    "\n",
    "        for features in image_features:\n",
    "            gen_volume = features.view(-1, 2048, 2, 2, 2)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 2048, 2, 2, 2])\n",
    "            gen_volume = self.layer1(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 512, 4, 4, 4])\n",
    "            gen_volume = self.layer2(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 128, 8, 8, 8])\n",
    "            gen_volume = self.layer3(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 32, 16, 16, 16])\n",
    "            gen_volume = self.layer4(gen_volume)\n",
    "            raw_feature = gen_volume\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 8, 32, 32, 32])\n",
    "            gen_volume = self.layer5(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 1, 32, 32, 32])\n",
    "            raw_feature = torch.cat((raw_feature, gen_volume), dim=1)\n",
    "            # print(raw_feature.size())  # torch.Size([batch_size, 9, 32, 32, 32])\n",
    "\n",
    "            gen_volumes.append(torch.squeeze(gen_volume, dim=1))\n",
    "            raw_features.append(raw_feature)\n",
    "\n",
    "        gen_volumes = torch.stack(gen_volumes).permute(1, 0, 2, 3, 4).contiguous()\n",
    "        raw_features = torch.stack(raw_features).permute(1, 0, 2, 3, 4, 5).contiguous()\n",
    "        # print(gen_volumes.size())      # torch.Size([batch_size, n_views, 32, 32, 32])\n",
    "        # print(raw_features.size())     # torch.Size([batch_size, n_views, 9, 32, 32, 32])\n",
    "        return raw_features, gen_volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:17:40.438206Z",
     "start_time": "2020-04-25T22:17:39.745734Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Decoder(cfg=None).cuda()\n",
    "# input size = output of encoder \n",
    "raw_features, gen_volumes = model(torch.Tensor(encoder_outputs.shape).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:11:30.153035Z",
     "start_time": "2020-04-25T22:11:30.141966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 9, 32, 32, 32])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:11:35.484722Z",
     "start_time": "2020-04-25T22:11:35.473469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32, 32, 32])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_volumes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:16:33.967144Z",
     "start_time": "2020-04-25T22:16:33.939319Z"
    }
   },
   "outputs": [],
   "source": [
    "class Merger(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Merger, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Layer Definition\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(9, 16, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm3d(16),\n",
    "            torch.nn.LeakyReLU(.2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(16, 8, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm3d(8),\n",
    "            torch.nn.LeakyReLU(.2)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(8, 4, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm3d(4),\n",
    "            torch.nn.LeakyReLU(.2)\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(4, 2, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm3d(2),\n",
    "            torch.nn.LeakyReLU(.2)\n",
    "        )\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(2, 1, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm3d(1),\n",
    "            torch.nn.LeakyReLU(.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, raw_features, coarse_volumes):\n",
    "        n_views_rendering = coarse_volumes.size(1)\n",
    "        raw_features = torch.split(raw_features, 1, dim=1)\n",
    "        volume_weights = []\n",
    "\n",
    "        for i in range(n_views_rendering):\n",
    "            raw_feature = torch.squeeze(raw_features[i], dim=1)\n",
    "            # print(raw_feature.size())       # torch.Size([batch_size, 9, 32, 32, 32])\n",
    "\n",
    "            volume_weight = self.layer1(raw_feature)\n",
    "            # print(volume_weight.size())     # torch.Size([batch_size, 16, 32, 32, 32])\n",
    "            volume_weight = self.layer2(volume_weight)\n",
    "            # print(volume_weight.size())     # torch.Size([batch_size, 8, 32, 32, 32])\n",
    "            volume_weight = self.layer3(volume_weight)\n",
    "            # print(volume_weight.size())     # torch.Size([batch_size, 4, 32, 32, 32])\n",
    "            volume_weight = self.layer4(volume_weight)\n",
    "            # print(volume_weight.size())     # torch.Size([batch_size, 2, 32, 32, 32])\n",
    "            volume_weight = self.layer5(volume_weight)\n",
    "            # print(volume_weight.size())     # torch.Size([batch_size, 1, 32, 32, 32])\n",
    "\n",
    "            volume_weight = torch.squeeze(volume_weight, dim=1)\n",
    "            # print(volume_weight.size())     # torch.Size([batch_size, 32, 32, 32])\n",
    "            volume_weights.append(volume_weight)\n",
    "\n",
    "        volume_weights = torch.stack(volume_weights).permute(1, 0, 2, 3, 4).contiguous()\n",
    "        volume_weights = torch.softmax(volume_weights, dim=1)\n",
    "        # print(volume_weights.size())        # torch.Size([batch_size, n_views, 32, 32, 32])\n",
    "        # print(coarse_volumes.size())        # torch.Size([batch_size, n_views, 32, 32, 32])\n",
    "        coarse_volumes = coarse_volumes * volume_weights\n",
    "        coarse_volumes = torch.sum(coarse_volumes, dim=1)\n",
    "\n",
    "        return torch.clamp(coarse_volumes, min=0, max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:20:20.278169Z",
     "start_time": "2020-04-25T22:20:20.248921Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Merger(cfg=None).cuda()\n",
    "merger_volumns = model(torch.Tensor(raw_features.shape).cuda(), torch.Tensor(gen_volumes.shape).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:19:24.058725Z",
     "start_time": "2020-04-25T22:19:24.027464Z"
    }
   },
   "outputs": [],
   "source": [
    "class Refiner(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Refiner, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Layer Definition\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(1, 32, kernel_size=4, padding=2),\n",
    "            torch.nn.BatchNorm3d(32),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.MaxPool3d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(32, 64, kernel_size=4, padding=2),\n",
    "            torch.nn.BatchNorm3d(64),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.MaxPool3d(kernel_size=2)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(64, 128, kernel_size=4, padding=2),\n",
    "            torch.nn.BatchNorm3d(128),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.MaxPool3d(kernel_size=2)\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8192, 2048),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2048, 8192),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm3d(64),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer7 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(64, 32, kernel_size=4, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm3d(32),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer8 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(32, 1, kernel_size=4, stride=2, bias=True, padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, coarse_volumes):\n",
    "        volumes_32_l = coarse_volumes.view((-1, 1, 32, 32, 32))\n",
    "        # print(volumes_32_l.size())       # torch.Size([batch_size, 1, 32, 32, 32])\n",
    "        volumes_16_l = self.layer1(volumes_32_l)\n",
    "        # print(volumes_16_l.size())       # torch.Size([batch_size, 32, 16, 16, 16])\n",
    "        volumes_8_l = self.layer2(volumes_16_l)\n",
    "        # print(volumes_8_l.size())        # torch.Size([batch_size, 64, 8, 8, 8])\n",
    "        volumes_4_l = self.layer3(volumes_8_l)\n",
    "        # print(volumes_4_l.size())        # torch.Size([batch_size, 128, 4, 4, 4])\n",
    "        flatten_features = self.layer4(volumes_4_l.view(-1, 8192))\n",
    "        # print(flatten_features.size())   # torch.Size([batch_size, 2048])\n",
    "        flatten_features = self.layer5(flatten_features)\n",
    "        # print(flatten_features.size())   # torch.Size([batch_size, 8192])\n",
    "        volumes_4_r = volumes_4_l + flatten_features.view(-1, 128, 4, 4, 4)\n",
    "        # print(volumes_4_r.size())        # torch.Size([batch_size, 128, 4, 4, 4])\n",
    "        volumes_8_r = volumes_8_l + self.layer6(volumes_4_r)\n",
    "        # print(volumes_8_r.size())        # torch.Size([batch_size, 64, 8, 8, 8])\n",
    "        volumes_16_r = volumes_16_l + self.layer7(volumes_8_r)\n",
    "        # print(volumes_16_r.size())       # torch.Size([batch_size, 32, 16, 16, 16])\n",
    "        volumes_32_r = (volumes_32_l + self.layer8(volumes_16_r)) * 0.5\n",
    "        # print(volumes_32_r.size())       # torch.Size([batch_size, 1, 32, 32, 32])\n",
    "\n",
    "        return volumes_32_r.view((-1, 32, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:23:36.926509Z",
     "start_time": "2020-04-25T22:23:36.550131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 32])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Refiner(cfg=None).cuda()\n",
    "refiner_outputs = model(torch.Tensor(merger_volumns.shape).cuda())\n",
    "refiner_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:40:16.720300Z",
     "start_time": "2020-04-25T22:40:16.699660Z"
    }
   },
   "outputs": [],
   "source": [
    "class Mapper(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Mapper, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 8, kernel_size=9, padding=1),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(0.2)\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(8, 8, kernel_size=4, stride=4, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.ConvTranspose2d(8, 8, kernel_size=2, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.ConvTranspose2d(8, 4, kernel_size=2, stride=2, bias=True, padding=1),\n",
    "            torch.nn.BatchNorm2d(4),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            torch.nn.ConvTranspose2d(4, 4, kernel_size=2, stride=2, bias=True, padding=2),\n",
    "            torch.nn.BatchNorm2d(4),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(4, 1, kernel_size=1, padding=0),\n",
    "            torch.nn.BatchNorm2d(1),\n",
    "            torch.nn.ReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, volumes):\n",
    "        batch_size = volumes.shape[0]\n",
    "        x = self.layer1(volumes)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:40:17.277380Z",
     "start_time": "2020-04-25T22:40:17.255749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 800, 800])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Mapper(cfg=None).cuda()\n",
    "model(torch.Tensor(refiner_outputs.shape).cuda()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:44:59.928569Z",
     "start_time": "2020-04-25T22:44:59.922636Z"
    }
   },
   "source": [
    "### implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:43:51.413855Z",
     "start_time": "2020-04-25T22:43:49.821264Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(cfg=None).cuda()\n",
    "decoder = Decoder(cfg=None).cuda()\n",
    "merger = Merger(cfg=None).cuda()\n",
    "refiner  = Refiner(cfg=None).cuda()\n",
    "mapper = Mapper(cfg=None).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:44:27.951108Z",
     "start_time": "2020-04-25T22:44:27.817154Z"
    }
   },
   "outputs": [],
   "source": [
    "# input size = [batch_size, n_views, img_c, img_h, img_w]\n",
    "encoder_outputs=encoder(torch.Tensor(1, 6, 3, 416, 416).cuda())\n",
    "raw_features, gen_volumes = decoder(encoder_outputs)\n",
    "merger_volumns = merger(raw_features, gen_volumes)\n",
    "refiner_columns = refiner(merger_volumns)\n",
    "outputs = mapper(refiner_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T22:44:36.398469Z",
     "start_time": "2020-04-25T22:44:36.387273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 800, 800])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See implementation details:\n",
    "\n",
    "https://github.com/hzxie/Pix2Vox/blob/f31ade6142f07e1d21c9c1c02b23e41410a9b230/core/train.py#L27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T23:01:07.944274Z",
     "start_time": "2020-04-25T23:01:07.921769Z"
    }
   },
   "outputs": [],
   "source": [
    "class pix2vox(torch.nn.Module):\n",
    "    def __init__(self, cfg, pretrained=True):\n",
    "        super(pix2vox, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.encoder = Encoder(self.cfg, pretrained=pretrained)\n",
    "        self.decoder = Decoder(self.cfg)\n",
    "        self.merger = Merger(self.cfg)\n",
    "        self.refiner = Refiner(self.cfg)\n",
    "        self.mapper = Mapper(self.cfg)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        encoder_outputs = encoder(inputs)\n",
    "        raw_features, gen_volumes = decoder(encoder_outputs)\n",
    "        merger_volumns = merger(raw_features, gen_volumes)\n",
    "        refiner_columns = refiner(merger_volumns)\n",
    "        outputs = mapper(refiner_columns)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def init_weights(m):\n",
    "        if type(m) == torch.nn.Conv2d or type(m) == torch.nn.Conv3d or type(m) == torch.nn.ConvTranspose3d:\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == torch.nn.BatchNorm2d or type(m) == torch.nn.BatchNorm3d:\n",
    "            torch.nn.init.constant_(m.weight, 1)\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "        elif type(m) == torch.nn.Linear:\n",
    "            torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "            torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T23:02:02.927517Z",
     "start_time": "2020-04-25T23:02:01.363863Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl = pix2vox(None, pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T23:02:24.849820Z",
     "start_time": "2020-04-25T23:02:24.463308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 1.4334, 1.7516,  ..., 0.5580, 0.0000, 1.1863],\n",
       "          [0.0000, 0.0000, 1.2064,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [1.7576, 2.0799, 0.0000,  ..., 3.8159, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.5333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6271, 0.6815, 0.0714,  ..., 3.9484, 1.0071, 0.0000],\n",
       "          [0.2754, 0.0000, 0.0833,  ..., 0.0000, 0.5345, 0.0000]]]],\n",
       "       device='cuda:0', grad_fn=<ReluBackward1>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl(torch.Tensor(1, 6, 3, 416, 416).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
